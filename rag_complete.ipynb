{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a41fa0ef",
   "metadata": {},
   "source": [
    "## Installation des d√©pendances\n",
    "\n",
    "Ex√©cutez cette cellule en premier pour installer tous les packages n√©cessaires. Cette op√©ration peut prendre quelques minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "988613cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.3)\n",
      "Collecting langchain-community\n",
      "  Using cached langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: langchain-text-splitters in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (5.1.2)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: rank-bm25 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.2.2)\n",
      "Requirement already satisfied: pypdf2 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (1.0.3)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (1.0.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (0.4.34)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (6.0.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (4.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain) (2.3)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.2)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.5.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (2023.5.7)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (2.0.38)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (3.12.13)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (2.11.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (3.1.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.57.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (0.35.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (70.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\moham\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\moham\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Using cached langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
      "Installing collected packages: langchain-community\n",
      "Successfully installed langchain-community-0.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-community langchain-text-splitters sentence-transformers faiss-cpu rank-bm25 pypdf2 python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7986c87d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.text_splitter'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext_splitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Pour langchain >=1.0.0\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# from langchain.text_splitters import RecursiveCharacterTextSplitter\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01membeddings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.text_splitter'"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# Pour langchain >=1.0.0\n",
    "# from langchain.text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers import EnsembleRetriever, BM25Retriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37472d37",
   "metadata": {},
   "source": [
    "# Projet RAG - Syst√®me de Recherche et G√©n√©ration Augment√©e\n",
    "\n",
    "Ce notebook impl√©mente un syst√®me RAG (Retrieval-Augmented Generation) pour r√©pondre √† des questions bas√©es sur des documents PDF de cours d'intelligence artificielle. Le syst√®me utilise des embeddings multilingues, un retriever hybride et un mod√®le de langage pour g√©n√©rer des r√©ponses pr√©cises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca45317",
   "metadata": {},
   "source": [
    "## √âtape 1 : Imports et configuration\n",
    "\n",
    "Dans cette √©tape, nous importons les biblioth√®ques n√©cessaires et configurons l'environnement. Nous chargeons √©galement la cl√© API OpenRouter depuis un fichier .env pour √©viter de l'exposer dans le code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1e9fc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moham\\OneDrive\\Bureau\\Rag-firstprtjt\\ai & applications prjt\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.chains'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FAISS\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchains\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RetrievalQA\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretrievers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BM25Retriever\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretrievers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EnsembleRetriever\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.chains'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# ‚úÖ Imports corrig√©s pour LangChain >= 1.0.0\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# üîë Cl√© OpenRouter\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "if not OPENROUTER_API_KEY:\n",
    "    raise ValueError(\"Merci de mettre ta cl√© OPENROUTER_API_KEY dans le fichier .env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2105134",
   "metadata": {},
   "source": [
    "## √âtape 2 : Chargement et extraction des textes des PDF\n",
    "\n",
    "Ici, nous parcourons le dossier 'data/' pour lire tous les fichiers PDF. Pour chaque PDF, nous extrayons le texte de toutes les pages, en nettoyant les espaces et les sauts de ligne. Les textes sont stock√©s dans une liste avec des m√©tadonn√©es sur la source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6750198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Charger les documents depuis le cache si disponible\n",
    "\n",
    "if os.path.exists(\"raw_documents.pkl\"):\n",
    "\n",
    "    with open(\"raw_documents.pkl\", \"rb\") as f:\n",
    "\n",
    "        raw_documents = pickle.load(f)\n",
    "\n",
    "    print(f\"Documents charg√©s depuis le cache : {len(raw_documents)} cours PDF.\")\n",
    "\n",
    "else:\n",
    "\n",
    "    folder_path = \"data/\"  # dossier contenant tous les PDFs\n",
    "\n",
    "    raw_documents = []\n",
    "\n",
    "    \n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            reader = PdfReader(pdf_path)\n",
    "\n",
    "            page_texts = []\n",
    "\n",
    "            for page in reader.pages:\n",
    "\n",
    "                content = (page.extract_text() or \"\").replace(\"\\n\", \" \").strip()\n",
    "\n",
    "                if content:\n",
    "\n",
    "                    page_texts.append(content)\n",
    "\n",
    "            full_text = \" \".join(page_texts)\n",
    "\n",
    "            if full_text:\n",
    "\n",
    "                raw_documents.append({\"text\": full_text, \"metadata\": {\"source\": filename}})\n",
    "\n",
    "    \n",
    "\n",
    "    if not raw_documents:\n",
    "\n",
    "        raise ValueError(\"Aucun texte extrait des PDFs. V√©rifie que le dossier data/ contient des fichiers PDF lisibles.\")\n",
    "\n",
    "    \n",
    "\n",
    "    # Sauvegarder les documents extraits\n",
    "\n",
    "    with open(\"raw_documents.pkl\", \"wb\") as f:\n",
    "\n",
    "        pickle.dump(raw_documents, f)\n",
    "\n",
    "    print(f\"Nombre de cours PDF exploitables : {len(raw_documents)} (sauvegard√© dans raw_documents.pkl)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab856304",
   "metadata": {},
   "source": [
    "## √âtape 3 : D√©coupage des textes en chunks\n",
    "\n",
    "Les textes longs sont divis√©s en morceaux plus petits (chunks) pour faciliter la recherche. Nous utilisons un splitter r√©cursif avec une taille de chunk de 350 caract√®res et un chevauchement de 120 caract√®res pour conserver le contexte entre les chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f863124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Charger les chunks depuis le cache si disponible\n",
    "\n",
    "if os.path.exists(\"documents_chunks.pkl\"):\n",
    "\n",
    "    with open(\"documents_chunks.pkl\", \"rb\") as f:\n",
    "\n",
    "        documents = pickle.load(f)\n",
    "\n",
    "    print(f\"Chunks charg√©s depuis le cache : {len(documents)} chunks.\")\n",
    "\n",
    "else:\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "\n",
    "        chunk_size=350,\n",
    "\n",
    "        chunk_overlap=120\n",
    "\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for item in raw_documents:\n",
    "\n",
    "        splitted_docs = text_splitter.create_documents([item[\"text\"]], metadatas=[item[\"metadata\"]])\n",
    "\n",
    "        for idx, doc in enumerate(splitted_docs):\n",
    "\n",
    "            doc.metadata[\"chunk_index\"] = idx\n",
    "\n",
    "        documents.extend(splitted_docs)\n",
    "\n",
    "    \n",
    "\n",
    "    if not documents:\n",
    "\n",
    "        raise ValueError(\"Aucun chunk g√©n√©r√©. V√©rifie les textes extraits.\")\n",
    "\n",
    "    \n",
    "\n",
    "    # Sauvegarder les chunks\n",
    "\n",
    "    with open(\"documents_chunks.pkl\", \"wb\") as f:\n",
    "\n",
    "        pickle.dump(documents, f)\n",
    "\n",
    "    print(f\"Total de chunks index√©s : {len(documents)} (sauvegard√© dans documents_chunks.pkl)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141153cc",
   "metadata": {},
   "source": [
    "## √âtape 4 : Cr√©ation des embeddings\n",
    "\n",
    "Nous cr√©ons des repr√©sentations vectorielles (embeddings) des chunks de texte en utilisant un mod√®le multilingue. Ces embeddings sont normalis√©s pour am√©liorer la similarit√© cosinus. Nous stockons ensuite ces vecteurs dans une base de donn√©es FAISS pour une recherche rapide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4706ef3",
   "metadata": {},
   "source": [
    "## Note importante sur la sauvegarde\n",
    "\n",
    "Les √©tapes pr√©c√©dentes sauvegardent automatiquement les r√©sultats interm√©diaires :\n",
    "- **raw_documents.pkl** : Documents extraits des PDF\n",
    "- **documents_chunks.pkl** : Chunks de texte d√©coup√©s\n",
    "- **faiss_index/** : Index FAISS avec les embeddings\n",
    "\n",
    "Si vous r√©ex√©cutez le notebook, ces fichiers seront charg√©s directement depuis le disque au lieu d'√™tre recalcul√©s, ce qui acc√©l√®re consid√©rablement le temps d'ex√©cution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fca59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "\n",
    "    model_name=\"intfloat/multilingual-e5-large\",\n",
    "\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Charger l'index FAISS depuis le disque si disponible\n",
    "\n",
    "if os.path.exists(\"faiss_index\"):\n",
    "\n",
    "    vectorstore = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n",
    "\n",
    "    print(\"Index FAISS charg√© depuis le disque.\")\n",
    "\n",
    "else:\n",
    "\n",
    "    vectorstore = FAISS.from_documents(documents, embedding_model)\n",
    "\n",
    "    print(\"Index FAISS construit avec embeddings e5.\")\n",
    "\n",
    "    # Sauvegarder l'index FAISS sur le disque pour √©viter de le recalculer\n",
    "\n",
    "    vectorstore.save_local(\"faiss_index\")\n",
    "\n",
    "    print(\"Index FAISS sauvegard√© dans le dossier 'faiss_index'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408d3b22",
   "metadata": {},
   "source": [
    "## √âtape 5 : Configuration du retriever hybride\n",
    "\n",
    "Pour am√©liorer la pr√©cision de la recherche, nous utilisons un retriever hybride qui combine la recherche dense (bas√©e sur les embeddings) et la recherche sparse (BM25, bas√©e sur les mots-cl√©s). Le retriever dense utilise MMR pour la diversit√©, et nous pond√©rons les r√©sultats avec 65% pour dense et 35% pour sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74d54bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",  # Maximum Marginal Relevance pour diversit√©\n",
    "    search_kwargs={\"k\": 3, \"fetch_k\": 4}\n",
    ")\n",
    "\n",
    "sparse_retriever = BM25Retriever.from_documents(documents)\n",
    "sparse_retriever.k = 8\n",
    "\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[dense_retriever, sparse_retriever],\n",
    "    weights=[0.65, 0.35]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a3365a",
   "metadata": {},
   "source": [
    "## √âtape 6 : Configuration du mod√®le de langage\n",
    "\n",
    "Nous configurons le mod√®le GPT-4o-mini via OpenRouter. La temp√©rature est r√©gl√©e √† 0.4 pour des r√©ponses plus factuelles. Nous utilisons des en-t√™tes personnalis√©s pour respecter les exigences d'OpenRouter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b61e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.4,  # temp√©rature plus basse pour des r√©ponses plus factuelles\n",
    "    openai_api_key=OPENROUTER_API_KEY,\n",
    "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "    default_headers={\n",
    "        \"HTTP-Referer\": \"https://github.com/your-user/your-repo\",\n",
    "        \"X-Title\": \"RAG Notebook\",\n",
    "    }\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=hybrid_retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type=\"stuff\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbace3ae",
   "metadata": {},
   "source": [
    "## √âtape 7 : Exemple de recherche et g√©n√©ration de r√©ponse\n",
    "\n",
    "Nous testons le syst√®me avec une question sur les d√©fis de l'intelligence artificielle. Le retriever r√©cup√®re les chunks pertinents, puis le mod√®le g√©n√®re une r√©ponse bas√©e sur ces informations. Les sources utilis√©es sont affich√©es pour la transparence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c361ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Expliquer les principaux d√©fis de l‚Äôintelligence artificielle\"\n",
    "\n",
    "result = qa_chain(query)\n",
    "\n",
    "print(\"R√©ponse du mod√®le :\")\n",
    "print(result[\"result\"])\n",
    "\n",
    "print(\"\\nSources utilis√©es :\")\n",
    "for i, doc in enumerate(result[\"source_documents\"]):\n",
    "    print(f\"Source {i} ({doc.metadata.get('source')}, chunk {doc.metadata.get('chunk_index')})\")\n",
    "    print(doc.page_content[:300], \"...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f737bb",
   "metadata": {},
   "source": [
    "## √âtape 8 : G√©n√©ration d'un quiz\n",
    "\n",
    "Enfin, nous demandons au syst√®me de g√©n√©rer un quiz bas√© sur les documents. Cela d√©montre la capacit√© du RAG √† cr√©er du contenu nouveau et pertinent √† partir des informations r√©cup√©r√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a2d849",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_quiz = \"\"\"\n",
    "G√©n√®re 5 questions de quiz avec leurs r√©ponses √† partir des passages pertinents\n",
    "des cours sur l‚Äôintelligence artificielle.\n",
    "\"\"\"\n",
    "\n",
    "result_quiz = qa_chain(query_quiz)\n",
    "\n",
    "print(\"Quiz g√©n√©r√© :\")\n",
    "print(result_quiz[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
